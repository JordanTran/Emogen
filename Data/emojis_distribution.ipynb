{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d272ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57479ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 5000\n",
    "tweets_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('a').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "\n",
    "tweets_df.to_csv('a', sep=',', index=False)\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('b').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('b', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('c').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('c', sep=',', index=False)\n",
    "\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('d').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('d', sep=',', index=False)\n",
    "\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('e').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('e', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('f').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('f', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('g').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('g', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('h').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('h', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('i').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('i', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('j').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('j', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('k').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('k', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('l').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('l', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('m').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('m', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('n').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('n', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('o').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('o', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('p').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('p', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('q').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('q', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('r').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('r', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('s').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('s', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('t').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('t', sep=',', index=False)\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('u').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('u', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('v').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('v', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('w').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('w', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('x').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('x', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('y').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('y', sep=',', index=False)\n",
    "\n",
    "\n",
    "tweets_list = []\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('z').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df.to_csv('z', sep=',', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728803f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from emojiList import all_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_tweet(lst):\n",
    "    l = []\n",
    "    for i in lst:\n",
    "        if set(all_emoji).intersection(tl[0]) != set():\n",
    "            l.append(i)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcb2473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2adb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/Users/zengspencer/Documents/DS-Project/tweets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(path + '/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2cfd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    l.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38520e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.concat(l, axis=0, ignore_index=True)\n",
    "tl = tweets['Text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec797470",
   "metadata": {},
   "outputs": [],
   "source": [
    "EmojiTweets = emoji_tweet(tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8125773",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(EmojiTweets,columns=['text'])\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94f07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacymoji import Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji = Emoji(nlp)\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7f75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojies(x):\n",
    "  doc = nlp(x['text']) \n",
    "  emojis = [token.text for token in doc if token._.is_emoji]\n",
    "  \n",
    "  return emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85cee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojies_df = df.apply(extract_emojies,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3615fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_counts = (emojies_df\n",
    "                .apply(pd.Series) \n",
    "                .stack() \n",
    "                .value_counts() \n",
    "                .rename('Count')\n",
    "                .sort_values()\n",
    "                .reset_index()\n",
    "                .rename(columns={'index':'Emoji'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0ae4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t60 = emoji_counts.tail(60)\n",
    "emoji_counts = t60.drop([1729,1723,1722,1711,1702,1687,1679,1678,1684,1713,1706,1685])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17560d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8018d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b0e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.add_trace(go.Scatter(x=emoji_counts['Emoji'],\n",
    "                y=emoji_counts['Count'],\n",
    "                name='Emoji Counts',\n",
    "                marker_color='white',\n",
    "                orientation='h',\n",
    "                text=emoji_counts['Emoji'],\n",
    "                textposition='top center',\n",
    "                mode='markers+text',\n",
    "                textfont=dict(size=30),\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4311a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.update_layout(width = 2470)\n",
    "fig.write_html(\"emoji_plot_udt.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
